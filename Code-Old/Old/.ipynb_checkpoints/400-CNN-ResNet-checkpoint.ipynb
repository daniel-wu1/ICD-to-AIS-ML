{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYS6016 Project: Deep learning to detection myocardial injury from initial ED ECG\n",
    "Team: Charlie\n",
    "Date: 5/7/21\n",
    "  \n",
    "This script contains implementation and training for our 1D ResNet model, as described by Park et al.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import sklearn\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from functools import partial\n",
    "from display_info import *\n",
    "from scipy.signal import butter, lfilter, freqz\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU was detected. CNNs can be very slow without a GPU.\n"
     ]
    }
   ],
   "source": [
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "#assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "    IS_COLAB = True\n",
    "except Exception:\n",
    "    IS_COLAB = False\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"No GPU was detected. CNNs can be very slow without a GPU.\")\n",
    "    if IS_COLAB:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
    "\n",
    "from tensorflow import Tensor\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, BatchNormalization,\\\n",
    "                                    Add, AveragePooling2D, Flatten, Dense, MaxPooling2D\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"/data/galen-ics333/ivy-hip-apid/dl_ecg/data/test_train_split/train/\"\n",
    "\n",
    "ecg_window = 640*4\n",
    "num_validation = 25\n",
    "batch_size = 32\n",
    "\n",
    "learning_rate = 0.00001\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_in_dir(file_dir):\n",
    "    filenames = []\n",
    "\n",
    "    for root, dirs, files in os.walk(os.path.abspath(file_dir)):\n",
    "        for file in files:\n",
    "            filenames.append(os.path.join(root, file))\n",
    "            \n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of ECGs in source dir\n",
    "ecg_files = get_files_in_dir(input_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ecgs(filesnames):\n",
    "    ecgs = []\n",
    "    mi = []\n",
    "\n",
    "    # get ecgs for these patients\n",
    "    for i, file in enumerate(filesnames):\n",
    "\n",
    "        try: \n",
    "            # read ECG file\n",
    "            ecg = pd.read_csv(file)\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(\"File not found:\", file)\n",
    "            continue\n",
    "\n",
    "        # convert panda to numpy\n",
    "        ecgs.append(ecg.T.values)\n",
    "\n",
    "        # capture mi labels\n",
    "        if \"positive\" in file:\n",
    "            mi.append(1)\n",
    "        else:\n",
    "            mi.append(0)\n",
    "    \n",
    "    # convert list to numpy\n",
    "    ecgs = np.asarray(ecgs)\n",
    "    mi = np.asarray(mi)\n",
    "    \n",
    "    # add one dimensional channel\n",
    "    #ecgs = np.expand_dims(ecgs,3)\n",
    "    \n",
    "    return np.asarray(ecgs), np.asarray(mi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.6 s, sys: 4.23 s, total: 34.9 s\n",
      "Wall time: 36.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get ecgs and mi data\n",
    "ecgs, mi = read_ecgs(ecg_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add extra dimension\n",
    "ecgs = np.expand_dims(ecgs,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get position of positive samples\n",
    "mi_neg_arg = np.squeeze(np.argwhere(mi==0))\n",
    "mi_pos_arg = np.squeeze(np.argwhere(mi==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample equal positives and negatives\n",
    "valid_neg = np.random.choice(mi_neg_arg, size=num_validation, replace=False)\n",
    "valid_pos = np.random.choice(mi_pos_arg, size=num_validation, replace=False)\n",
    "\n",
    "# combine and randomize order\n",
    "valid_args = np.random.choice(np.append(valid_neg, valid_pos), size=num_validation*2, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine training arguments and randomize order\n",
    "train_args = np.delete(np.arange(0,len(mi)),valid_args)\n",
    "train_args = np.random.choice(train_args, size=len(train_args), replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and validation\n",
    "train_ecg = ecgs[train_args]\n",
    "train_mi = mi[train_args]\n",
    "\n",
    "val_ecg = ecgs[valid_args]\n",
    "val_mi = mi[valid_args]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data_array, label_array, batch_size = 20, window_size=1000):\n",
    "    i = 0\n",
    "    indx = list(range(0,data_array.shape[0]))\n",
    "    np.random.shuffle(indx)\n",
    "    while True:\n",
    "        if i*batch_size >= len(indx):  # This loop is used to run the generator indefinitely.\n",
    "            i = 0\n",
    "            np.random.shuffle(indx)\n",
    "        else:\n",
    "            if batch_size > 0:\n",
    "                data_chunk = data_array[indx[i*batch_size:(i+1)*batch_size]] \n",
    "                label_chunk = label_array[indx[i*batch_size:(i+1)*batch_size]] \n",
    "            else:\n",
    "                data_chunk = data_array[indx]\n",
    "                label_chunk = label_array[indx]\n",
    "\n",
    "            # window sample\n",
    "            start = np.random.randint(0,data_array.shape[2]-window_size-500)\n",
    "            data_chunk = data_chunk[:,:,start:start+window_size]\n",
    "                \n",
    "            yield data_chunk, label_chunk\n",
    "            i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = data_generator(train_ecg, train_mi, batch_size=batch_size, window_size=ecg_window)\n",
    "val_ds = data_generator(val_ecg, val_mi, batch_size=num_validation*2, window_size=ecg_window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_bn(inputs: Tensor) -> Tensor:\n",
    "    relu = ReLU()(inputs)\n",
    "    bn = BatchNormalization()(relu)\n",
    "    return bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(x, downsample, filters, kernel_size=(1,3)) -> Tensor:\n",
    "    y = Conv2D(kernel_size=kernel_size,\n",
    "               strides= (1 if not downsample else 2),\n",
    "               filters=filters,\n",
    "               padding=\"same\")(x)\n",
    "    y = relu_bn(y)\n",
    "    y = Conv2D(kernel_size=kernel_size,\n",
    "               strides=1,\n",
    "               filters=filters,\n",
    "               padding=\"same\")(y)\n",
    "\n",
    "    if downsample:\n",
    "        x = Conv2D(kernel_size=1,\n",
    "                   strides=2,\n",
    "                   filters=filters,\n",
    "                   padding=\"same\")(x)\n",
    "    out = Add()([x, y])\n",
    "    out = relu_bn(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_res_net():\n",
    "    \n",
    "    inputs = Input(shape=[12, ecg_window, 1])\n",
    "    num_filters = 64\n",
    "\n",
    "    t = Conv2D(kernel_size=(1,7),\n",
    "               strides=1,\n",
    "               filters=num_filters,\n",
    "               padding=\"same\")(inputs)\n",
    "    \n",
    "    t = BatchNormalization()(t)\n",
    "\n",
    "    t = relu_bn(t)\n",
    "    \n",
    "    t = MaxPooling2D(pool_size=(1,2))(t)\n",
    "    \n",
    "    num_blocks_list = [3, 4, 6, 3]\n",
    "    for i in range(len(num_blocks_list)):\n",
    "        num_blocks = num_blocks_list[i]\n",
    "        for j in range(num_blocks):\n",
    "            t = residual_block(t, downsample=(j==0 and i!=0), filters=num_filters)\n",
    "        num_filters *= 2\n",
    "        \n",
    "    t = AveragePooling2D(1,4)(t)\n",
    "    t = Flatten()(t)\n",
    "    outputs = Dense(2, activation='softmax')(t)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_res_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_rat = sum(train_mi)/len(train_mi)\n",
    "class_weights = {0: mi_rat,\n",
    "                1: 1-mi_rat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "64/64 [==============================] - 689s 11s/step - loss: 0.1743 - accuracy: 0.4358 - val_loss: 0.7142 - val_accuracy: 0.5400\n",
      "Epoch 2/20\n",
      "64/64 [==============================] - 682s 11s/step - loss: 0.1820 - accuracy: 0.4850 - val_loss: 0.7159 - val_accuracy: 0.5000\n",
      "Epoch 3/20\n",
      "64/64 [==============================] - 682s 11s/step - loss: 0.1862 - accuracy: 0.4661 - val_loss: 0.7691 - val_accuracy: 0.4600\n",
      "Epoch 4/20\n",
      "64/64 [==============================] - 684s 11s/step - loss: 0.1874 - accuracy: 0.4613 - val_loss: 0.7513 - val_accuracy: 0.5400\n",
      "Epoch 5/20\n",
      "64/64 [==============================] - 685s 11s/step - loss: 0.1762 - accuracy: 0.4870 - val_loss: 0.7268 - val_accuracy: 0.5800\n",
      "Epoch 6/20\n",
      "64/64 [==============================] - 686s 11s/step - loss: 0.1693 - accuracy: 0.5210 - val_loss: 0.7062 - val_accuracy: 0.5200\n",
      "Epoch 7/20\n",
      "64/64 [==============================] - 686s 11s/step - loss: 0.1726 - accuracy: 0.5021 - val_loss: 0.8225 - val_accuracy: 0.4600\n",
      "Epoch 8/20\n",
      "64/64 [==============================] - 683s 11s/step - loss: 0.1691 - accuracy: 0.4967 - val_loss: 0.7481 - val_accuracy: 0.4000\n",
      "Epoch 9/20\n",
      "64/64 [==============================] - 683s 11s/step - loss: 0.1694 - accuracy: 0.4899 - val_loss: 0.7556 - val_accuracy: 0.5200\n",
      "Epoch 10/20\n",
      "64/64 [==============================] - 683s 11s/step - loss: 0.1754 - accuracy: 0.5218 - val_loss: 0.7709 - val_accuracy: 0.5600\n",
      "Epoch 11/20\n",
      "64/64 [==============================] - 683s 11s/step - loss: 0.1883 - accuracy: 0.4742 - val_loss: 0.8775 - val_accuracy: 0.4000\n",
      "Epoch 12/20\n",
      "64/64 [==============================] - 681s 11s/step - loss: 0.1673 - accuracy: 0.4903 - val_loss: 0.6645 - val_accuracy: 0.5800\n",
      "Epoch 13/20\n",
      "64/64 [==============================] - 683s 11s/step - loss: 0.1780 - accuracy: 0.4799 - val_loss: 0.7856 - val_accuracy: 0.5000\n",
      "Epoch 14/20\n",
      "64/64 [==============================] - 685s 11s/step - loss: 0.1715 - accuracy: 0.4926 - val_loss: 0.8100 - val_accuracy: 0.4200\n",
      "Epoch 15/20\n",
      "64/64 [==============================] - 685s 11s/step - loss: 0.1760 - accuracy: 0.4693 - val_loss: 0.8607 - val_accuracy: 0.5200\n",
      "Epoch 16/20\n",
      "64/64 [==============================] - 683s 11s/step - loss: 0.1642 - accuracy: 0.5091 - val_loss: 0.7970 - val_accuracy: 0.5000\n",
      "Epoch 17/20\n",
      "64/64 [==============================] - 683s 11s/step - loss: 0.1641 - accuracy: 0.5051 - val_loss: 0.8613 - val_accuracy: 0.3400\n",
      "Epoch 18/20\n",
      "64/64 [==============================] - 684s 11s/step - loss: 0.1793 - accuracy: 0.4915 - val_loss: 0.7716 - val_accuracy: 0.5800\n",
      "Epoch 19/20\n",
      "64/64 [==============================] - 685s 11s/step - loss: 0.1675 - accuracy: 0.5029 - val_loss: 0.6693 - val_accuracy: 0.6200\n",
      "Epoch 20/20\n",
      "64/64 [==============================] - 684s 11s/step - loss: 0.1705 - accuracy: 0.4889 - val_loss: 0.8901 - val_accuracy: 0.3600\n",
      "CPU times: user 12h 50min 52s, sys: 1h 7min 31s, total: 13h 58min 23s\n",
      "Wall time: 3h 48min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = model.fit(train_ds, \n",
    "                    validation_data=val_ds, \n",
    "                    epochs=20,\n",
    "                    steps_per_epoch = int(len(train_mi)/batch_size)+1,\n",
    "                    validation_steps = 1,\n",
    "                    shuffle=True, \n",
    "                    class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positives:  22\n"
     ]
    }
   ],
   "source": [
    "val_dat, val_mi = next(val_ds)\n",
    "pred = model.predict(val_dat)\n",
    "pred_int = np.argmax(pred,axis=1)\n",
    "print(\"Positives: \", sum(pred_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0   0   1\n",
       "row_0        \n",
       "0      14  11\n",
       "1      14  11"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(val_mi, pred_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model with test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "ecg_files = get_files_in_dir(\"/data/galen-ics333/ivy-hip-apid/dl_ecg/data/test_train_split/test/\")\n",
    "ex_dat, ex_mi = read_ecgs(ecg_files)\n",
    "\n",
    "ex_dat = np.expand_dims(ex_dat,3)\n",
    "\n",
    "ex_ds,ex_mi = next(data_generator(ex_dat, ex_mi, batch_size=len(ex_mi), window_size=ecg_window))\n",
    "\n",
    "pred_ex = model.predict(ex_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positives:  32\n"
     ]
    }
   ],
   "source": [
    "pred_ex_int = np.argmax(pred_ex,axis=1)\n",
    "print(\"Positives: \", sum(pred_ex_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0   0   1\n",
       "row_0        \n",
       "0      16  14\n",
       "1      12  18"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(ex_mi, pred_ex_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /data/galen-ics333/ivy-hip-apid/dl_ecg/models/shanghai_resnet2/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"/data/galen-ics333/ivy-hip-apid/dl_ecg/models/shanghai_resnet2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
